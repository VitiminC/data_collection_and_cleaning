{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pathlib\n",
    "#!pip install praw==7.5.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e97e1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import nltk.corpus\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from profanity_filter import ProfanityFilter\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d1c2e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title of csv to be cleaned:ThatsInsane\n",
      "dupe removal failed\n",
      "0 out of 196\n",
      "1 out of 196\n",
      "2 out of 196\n",
      "3 out of 196\n",
      "dupe removal failed\n",
      "4 out of 196\n",
      "5 out of 196\n",
      "dupe removal failed\n",
      "6 out of 196\n",
      "7 out of 196\n",
      "8 out of 196\n",
      "9 out of 196\n",
      "10 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "11 out of 196\n",
      "12 out of 196\n",
      "13 out of 196\n",
      "14 out of 196\n",
      "15 out of 196\n",
      "16 out of 196\n",
      "17 out of 196\n",
      "18 out of 196\n",
      "19 out of 196\n",
      "20 out of 196\n",
      "21 out of 196\n",
      "22 out of 196\n",
      "dupe removal failed\n",
      "23 out of 196\n",
      "24 out of 196\n",
      "25 out of 196\n",
      "26 out of 196\n",
      "dupe removal failed\n",
      "27 out of 196\n",
      "28 out of 196\n",
      "29 out of 196\n",
      "30 out of 196\n",
      "31 out of 196\n",
      "32 out of 196\n",
      "33 out of 196\n",
      "34 out of 196\n",
      "35 out of 196\n",
      "36 out of 196\n",
      "37 out of 196\n",
      "38 out of 196\n",
      "39 out of 196\n",
      "40 out of 196\n",
      "41 out of 196\n",
      "42 out of 196\n",
      "43 out of 196\n",
      "44 out of 196\n",
      "dupe removal failed\n",
      "45 out of 196\n",
      "46 out of 196\n",
      "47 out of 196\n",
      "48 out of 196\n",
      "49 out of 196\n",
      "50 out of 196\n",
      "51 out of 196\n",
      "52 out of 196\n",
      "53 out of 196\n",
      "dupe removal failed\n",
      "54 out of 196\n",
      "dupe removal failed\n",
      "55 out of 196\n",
      "56 out of 196\n",
      "57 out of 196\n",
      "58 out of 196\n",
      "59 out of 196\n",
      "60 out of 196\n",
      "61 out of 196\n",
      "62 out of 196\n",
      "63 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "64 out of 196\n",
      "65 out of 196\n",
      "66 out of 196\n",
      "dupe removal failed\n",
      "67 out of 196\n",
      "dupe removal failed\n",
      "68 out of 196\n",
      "69 out of 196\n",
      "70 out of 196\n",
      "dupe removal failed\n",
      "71 out of 196\n",
      "dupe removal failed\n",
      "72 out of 196\n",
      "dupe removal failed\n",
      "73 out of 196\n",
      "74 out of 196\n",
      "75 out of 196\n",
      "76 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "77 out of 196\n",
      "78 out of 196\n",
      "79 out of 196\n",
      "80 out of 196\n",
      "81 out of 196\n",
      "82 out of 196\n",
      "dupe removal failed\n",
      "83 out of 196\n",
      "dupe removal failed\n",
      "84 out of 196\n",
      "85 out of 196\n",
      "dupe removal failed\n",
      "86 out of 196\n",
      "87 out of 196\n",
      "88 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "89 out of 196\n",
      "90 out of 196\n",
      "91 out of 196\n",
      "dupe removal failed\n",
      "92 out of 196\n",
      "dupe removal failed\n",
      "93 out of 196\n",
      "94 out of 196\n",
      "dupe removal failed\n",
      "95 out of 196\n",
      "96 out of 196\n",
      "97 out of 196\n",
      "98 out of 196\n",
      "99 out of 196\n",
      "100 out of 196\n",
      "101 out of 196\n",
      "102 out of 196\n",
      "103 out of 196\n",
      "104 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "105 out of 196\n",
      "106 out of 196\n",
      "107 out of 196\n",
      "dupe removal failed\n",
      "108 out of 196\n",
      "109 out of 196\n",
      "110 out of 196\n",
      "111 out of 196\n",
      "112 out of 196\n",
      "dupe removal failed\n",
      "113 out of 196\n",
      "114 out of 196\n",
      "115 out of 196\n",
      "116 out of 196\n",
      "dupe removal failed\n",
      "117 out of 196\n",
      "dupe removal failed\n",
      "118 out of 196\n",
      "119 out of 196\n",
      "120 out of 196\n",
      "121 out of 196\n",
      "122 out of 196\n",
      "123 out of 196\n",
      "124 out of 196\n",
      "dupe removal failed\n",
      "125 out of 196\n",
      "126 out of 196\n",
      "127 out of 196\n",
      "dupe removal failed\n",
      "128 out of 196\n",
      "129 out of 196\n",
      "130 out of 196\n",
      "dupe removal failed\n",
      "131 out of 196\n",
      "132 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "133 out of 196\n",
      "134 out of 196\n",
      "135 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "136 out of 196\n",
      "137 out of 196\n",
      "138 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "139 out of 196\n",
      "140 out of 196\n",
      "141 out of 196\n",
      "142 out of 196\n",
      "dupe removal failed\n",
      "143 out of 196\n",
      "144 out of 196\n",
      "145 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "146 out of 196\n",
      "147 out of 196\n",
      "148 out of 196\n",
      "149 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "150 out of 196\n",
      "151 out of 196\n",
      "152 out of 196\n",
      "dupe removal failed\n",
      "dupe removal failed\n",
      "153 out of 196\n",
      "dupe removal failed\n",
      "154 out of 196\n",
      "155 out of 196\n",
      "156 out of 196\n",
      "157 out of 196\n",
      "158 out of 196\n",
      "159 out of 196\n",
      "160 out of 196\n",
      "161 out of 196\n",
      "162 out of 196\n",
      "163 out of 196\n",
      "164 out of 196\n",
      "165 out of 196\n",
      "166 out of 196\n",
      "167 out of 196\n",
      "168 out of 196\n",
      "169 out of 196\n",
      "170 out of 196\n",
      "171 out of 196\n",
      "172 out of 196\n",
      "173 out of 196\n",
      "174 out of 196\n",
      "175 out of 196\n",
      "176 out of 196\n",
      "177 out of 196\n",
      "dupe removal failed\n",
      "178 out of 196\n",
      "179 out of 196\n",
      "180 out of 196\n",
      "181 out of 196\n",
      "182 out of 196\n",
      "183 out of 196\n",
      "184 out of 196\n",
      "185 out of 196\n",
      "dupe removal failed\n",
      "186 out of 196\n",
      "187 out of 196\n",
      "188 out of 196\n",
      "dupe removal failed\n",
      "189 out of 196\n",
      "190 out of 196\n",
      "191 out of 196\n",
      "dupe removal failed\n",
      "192 out of 196\n",
      "dupe removal failed\n",
      "193 out of 196\n",
      "194 out of 196\n",
      "195 out of 196\n",
      "done\n",
      "ThatsInsane\n"
     ]
    }
   ],
   "source": [
    "class Cleaner:\n",
    "    def __init__(self):\n",
    "        self.delete_list = ['[deleted]','[removed]','removed','deleted','https', 'Repost','repost','!gif','&#x20B','[OC]','*',\n",
    "                            'instagram','facebook','twitter','google', '/r', '/u', 'http', 'www', '/s', '#']\n",
    "        self.sub_list = [r'r/\\S+',r'\\([^)]*\\)',r'http\\S+',r'[()]',r'[:]',r'[\\([{})\\]]']\n",
    "        self.pf = ProfanityFilter()\n",
    "        #self.cols = ['title', 'image_link', 'subreddit']\n",
    "        self.cols = ['title', 'image_link']\n",
    "    \n",
    "    #Removes emoticons\n",
    "    def remove_emojis(data):\n",
    "        emoj = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\" \n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"  # dingbats\n",
    "            u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "        return re.sub(emoj, '', data)\n",
    "\n",
    "    #removes characters that appear more than twice ex: booo -> boo\n",
    "    keep_list = ['s','e','t','f','l','m','o','p','d','n','g','r','b']\n",
    "    def remove_dupes(s):\n",
    "        ans = \"\"\n",
    "        seen = ''\n",
    "        i = 0\n",
    "        while i != (len(s)-1):\n",
    "            if s[i] in keep_list and s[i] != seen:\n",
    "                if s[i] == s[i+1]:\n",
    "                    seen = s[i]\n",
    "                    ans+= s[i]\n",
    "                    ans+= s[i+1]\n",
    "            if s[i] != seen and s[i+1] != s[i]:\n",
    "                seen = s[i]\n",
    "                ans += s[i]\n",
    "                i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        ans += s[len(s)-1]\n",
    "        if ans[-1] == ans[-2] and ans[-1] == ans[-2]:\n",
    "            ans = ans[0:-1]\n",
    "        return ans\n",
    "\n",
    "    def splitter(text, span):\n",
    "        s = span\n",
    "        words = re.sub(r'[^\\w\\s]', '', text).split(' ')\n",
    "        t = [\" \".join(words[i:i+s]) for i in range(0, len(words), s)]\n",
    "        return t\n",
    "    \n",
    "    #Subsets dataset and define filters\n",
    "    def clean(self, DATA, subreddit):\n",
    "        data = pd.read_csv(DATA,sep=\",\", encoding='utf-8',on_bad_lines='skip')\n",
    "        data_subset = data.drop(self.cols, axis=1)\n",
    "        data_first = pd.read_csv(DATA,sep=\",\", encoding='utf-8',on_bad_lines='skip')[self.cols]\n",
    "        for i in range(len(data_subset)):\n",
    "        #for i in range(1,4):\n",
    "            for j in range(len(data_subset.iloc[i])):\n",
    "                text = data_subset.iloc[i][j]\n",
    "                try:\n",
    "                    text = remove_emojis(text)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "                try:\n",
    "                    text = remove_dupes(text)\n",
    "                except:\n",
    "                    print('dupe removal failed')\n",
    "                    continue\n",
    "                text = text.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"_x000D_\", \" \").strip()\n",
    "                try:\n",
    "                    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "                    for sub in self.sub_list:\n",
    "                        text = re.sub(sub,'',text)\n",
    "                except:\n",
    "                    continue\n",
    "                #text = self.pf.censor(text)\n",
    "                if any(word in data_subset.iloc[i][j].lower() for word in self.delete_list):\n",
    "                    text = 0\n",
    "                #data_subset['comment' + str(j+1)][i] = text\n",
    "                data_subset['comment' + str(j)][i] = text\n",
    "            print(i,\"out of\",len(data_subset))\n",
    "    \n",
    "\n",
    "        #format data\n",
    "        result = pd.concat([data_first, data_subset], axis=1, join='inner')\n",
    "        result = result.iloc[1:, :]\n",
    "        df = pd.DataFrame(columns = ['SubmissionID', 'SubmissionTitle', 'CommentID', 'Comment', 'subreddit', 'Images'])\n",
    "        for i in range(len(result)):\n",
    "            commentid = 1\n",
    "            title = result.iloc[i][0]\n",
    "            #sub = result.iloc[i][2]\n",
    "            sub = subreddit\n",
    "            image = result.iloc[i][1]\n",
    "            for j in result.iloc[i][4:]:\n",
    "                if j == 'fill':\n",
    "                    continue\n",
    "                elif j != 0 and len(str(j)) > 0:\n",
    "                    insert = ['placeholderID', title, commentid, j, sub, image]\n",
    "                    df = df.append(pd.DataFrame([insert],\n",
    "                          columns = ['SubmissionID', 'SubmissionTitle', 'CommentID', 'Comment', 'subreddit', 'Images']))\n",
    "                    commentid += 1\n",
    "        return df\n",
    "\n",
    "obj = Cleaner()\n",
    "subreddit = input('title of csv to be cleaned:')\n",
    "#data = f'data/raw_data/{subreddit}.csv'\n",
    "data = f'data/raw_data/1/{subreddit}.csv'\n",
    "res = obj.clean(data, subreddit)\n",
    "res.to_csv(f'data/cleaned_data/{subreddit}.csv', index=False, encoding='utf8')\n",
    "print('done')\n",
    "print(subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482419a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa811501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
